---
title: "Pose Comparison Library"
description: "Reference-based pose matching and similarity scoring"
---

The `poseComparison.ts` library compares user poses against reference poses from videos, enabling move-by-move feedback and color-coded skeleton overlays.

## Types

### PoseComparisonResult

```typescript
type PoseComparisonResult = {
  overallScore: number;                    // 0-100 match score
  connectionColors: Map<string, string>;   // Per-connection colors for drawSkeleton
};
```

Result from `comparePoses()`. Use `connectionColors` with `drawSkeleton()` to render color-coded feedback.

### DetailedComparison

```typescript
type DetailedComparison = {
  matchScore: number;                     // Overall 0-100 score
  limbScores: Record<string, number>;     // Per-limb scores
  worstLimb: string;                      // Limb with lowest score
  refPoseLabel: string;                   // Classified pose name
};
```

Detailed breakdown from `comparePosesDetailed()`. Includes per-limb analysis for targeted feedback.

## Constants

### Color Thresholds

```typescript
const COLOR_GREEN = "#22c55e";   // Good match (< 6% error)
const COLOR_YELLOW = "#eab308";  // OK match (6-12% error)
const COLOR_RED = "#ef4444";     // Poor match (> 12% error)

const THRESH_GOOD = 0.06;        // Green threshold (normalized units)
const THRESH_OK = 0.12;          // Yellow threshold (normalized units)
```

### Limb Groups

```typescript
const LIMB_JOINTS: Record<string, number[]> = {
  rightArm: [12, 14, 16],   // Right shoulder, elbow, wrist
  leftArm: [11, 13, 15],    // Left shoulder, elbow, wrist
  rightLeg: [24, 26, 28],   // Right hip, knee, ankle
  leftLeg: [23, 25, 27],    // Left hip, knee, ankle
  torso: [11, 12, 23, 24]   // Shoulders and hips
};
```

Limb groupings for per-limb scoring. Each limb maps to its constituent landmark indices.

## Functions

### normalizePose

```typescript
function normalizePose(
  landmarks: NormalizedLandmark[],
  aspectRatio: number = 1
): NormalizedLandmark[]
```

Normalizes a pose so reference and live skeletons align visually. Essential for comparing poses from different video sources (e.g., 16:9 YouTube video vs. 4:3 webcam).

**Parameters:**
- `landmarks`: Raw MediaPipe landmarks
- `aspectRatio`: width/height of source video (default: 1 for square)

**Algorithm:**

1. **Aspect ratio correction**: Multiply x-coordinates by aspectRatio to map to square space
2. **Centering**: Align hip midpoint to (0.5, 0.6)
3. **Scaling**: Scale by torso length (target: 0.25 normalized units)

**Formula:**
```typescript
// 1. Aspect ratio correction
correctedX = x * aspectRatio

// 2. Find hip midpoint
hipMidX = (leftHipX + rightHipX) / 2
hipMidY = (leftHipY + rightHipY) / 2

// 3. Compute torso length
torsoLen = distance(hipMid, shoulderMid)

// 4. Scale to target torso length
scale = 0.25 / torsoLen

// 5. Center at (0.5, 0.6)
normalizedX = (correctedX - hipMidX) * scale + 0.5
normalizedY = (correctedY - hipMidY) * scale + 0.6
```

**Usage:**
```typescript
import { normalizePose } from "@/lib/poseComparison";

// Reference pose from 16:9 video
const refNormalized = normalizePose(refLandmarks, 16/9);

// Live pose from 4:3 webcam
const liveNormalized = normalizePose(liveLandmarks, 4/3);

// Now comparable!
```

### comparePoses

```typescript
function comparePoses(
  ref: NormalizedLandmark[],
  live: NormalizedLandmark[]
): PoseComparisonResult
```

Compares two normalized poses and returns an overall score plus per-connection color map.

**Parameters:**
- `ref`: Reference pose (already normalized)
- `live`: Live pose (already normalized)

**Returns:**
- `overallScore`: 0-100 match score
- `connectionColors`: Map of `"${a}-${b}"` → color string

**Algorithm:**

1. **Per-joint distance**: Measure Euclidean distance for each joint
2. **Group by limb**: Average distances within each limb
3. **Color mapping**: Assign green/yellow/red based on thresholds
4. **Overall score**: Average all limb scores

**Reference-centric**: Only compares landmarks visible in the reference pose:
- If reference has landmark but user doesn't: maximum penalty
- If user has landmark but reference doesn't: ignored

**Usage:**
```typescript
import { comparePoses, normalizePose } from "@/lib/poseComparison";
import { drawSkeleton } from "@/lib/pose";

// Normalize both poses first
const refNorm = normalizePose(refLandmarks, 16/9);
const liveNorm = normalizePose(liveLandmarks, 4/3);

// Compare
const { overallScore, connectionColors } = comparePoses(refNorm, liveNorm);

console.log(`Match: ${overallScore}%`);

// Draw with color-coded feedback
drawSkeleton(ctx, liveLandmarks, width, height, {
  connectionColors,
  strokeColor: "#808080"  // Fallback for unmapped connections
});
```

**Color interpretation:**
- **Green**: Excellent match (< 6% normalized error)
- **Yellow**: Acceptable match (6-12% error)
- **Red**: Needs improvement (> 12% error)

### comparePosesDetailed

```typescript
function comparePosesDetailed(
  refLandmarks: NormalizedLandmark[],
  liveLandmarks: NormalizedLandmark[],
  refAspectRatio: number = 1,
  liveAspectRatio: number = 1
): DetailedComparison | null
```

Detailed comparison with per-limb breakdown and pose classification.

**Parameters:**
- `refLandmarks`: Reference landmarks (unnormalized)
- `liveLandmarks`: Live landmarks (unnormalized)
- `refAspectRatio`: Reference video aspect ratio
- `liveAspectRatio`: Live video aspect ratio

**Returns:**
- `matchScore`: Overall 0-100 score
- `limbScores`: Per-limb scores (rightArm, leftArm, rightLeg, leftLeg, torso)
- `worstLimb`: Limb with lowest score
- `refPoseLabel`: Human-readable pose name (e.g., "Arms up", "Star jump")

**Usage:**
```typescript
import { comparePosesDetailed } from "@/lib/poseComparison";

const detailed = comparePosesDetailed(
  refLandmarks,
  liveLandmarks,
  16/9,  // YouTube video
  4/3    // Webcam
);

if (detailed) {
  console.log(`Trying to match: ${detailed.refPoseLabel}`);
  console.log(`Overall: ${detailed.matchScore}%`);
  console.log(`Worst limb: ${detailed.worstLimb} (${detailed.limbScores[detailed.worstLimb]}%)`);
}
```

**Example output:**
```typescript
{
  matchScore: 78,
  limbScores: {
    rightArm: 85,
    leftArm: 72,
    rightLeg: 80,
    leftLeg: 76,
    torso: 82
  },
  worstLimb: "leftArm",
  refPoseLabel: "Arms up"
}
```

### classifyPose

```typescript
function classifyPose(
  landmarks: NormalizedLandmark[]
): string
```

Classifies a pose into a human-readable label based on landmark geometry.

**Detected poses:**
- `"Star jump"`: Arms up + wide stance
- `"Arms up"`: Both wrists above shoulders
- `"T-pose"`: Arms wide + wide stance
- `"Arms wide"`: Arms spread horizontally
- `"Left reach"`: Left arm up, right arm down
- `"Right reach"`: Right arm up, left arm down
- `"Low squat"`: Crouching + wide stance
- `"Crouch"`: Hips below neutral
- `"Wide stance"`: Ankles far apart
- `"Neutral"`: Default pose
- `"Pose"`: Fallback if shoulders/hips missing

**Usage:**
```typescript
import { classifyPose } from "@/lib/poseComparison";

const label = classifyPose(landmarks);
console.log(`Current pose: ${label}`);
```

### findClosestFrame

```typescript
function findClosestFrame(
  timeline: { time: number; landmarks: NormalizedLandmark[] }[],
  time: number
): { time: number; landmarks: NormalizedLandmark[] } | null
```

Finds the closest frame in a pose timeline to a given timestamp. Used to sync reference poses with video playback.

**Parameters:**
- `timeline`: Array of timestamped pose frames
- `time`: Target timestamp in seconds

**Returns:**
- Closest frame with non-empty landmarks, or null if timeline is empty

**Algorithm:**
- Linear search for minimum absolute time difference
- Filters out frames with no landmarks

**Usage:**
```typescript
import { findClosestFrame } from "@/lib/poseComparison";
import type { PoseTimeline } from "@/lib/videoPoseExtractor";

function syncReferencePose(
  refTimeline: PoseTimeline,
  videoTime: number
) {
  const frame = findClosestFrame(refTimeline, videoTime);
  if (frame) {
    // Use frame.landmarks for comparison
    return frame.landmarks;
  }
  return null;
}
```

## Scoring Algorithm

### Distance Calculation

```typescript
function dist(a: NormalizedLandmark, b: NormalizedLandmark): number {
  return Math.sqrt((a.x - b.x)² + (a.y - b.y)²);
}
```

Euclidean distance in normalized coordinate space (after aspect ratio correction).

### Limb Score Calculation

```typescript
// For each limb:
const distances: number[] = [];

for (const jointIdx of limbJoints) {
  const refJoint = refNorm[jointIdx];
  const liveJoint = liveNorm[jointIdx];

  // Skip if reference doesn't have this joint
  if (!refJoint || refJoint.visibility < 0.3) continue;

  // Reference has joint - check user
  if (!liveJoint || liveJoint.visibility < 0.3) {
    distances.push(THRESH_OK * 2);  // Maximum penalty
  } else {
    distances.push(dist(refJoint, liveJoint));
  }
}

// Average distance → score
const avgDist = distances.reduce((s, d) => s + d, 0) / distances.length;
const limbScore = Math.max(0, Math.min(100, 100 * (1 - avgDist / (THRESH_OK * 2))));
```

### Overall Score

```typescript
overallScore = average(limbScores)  // Rounded to integer
```

## Integration Example

Complete example with video sync:

```typescript
import { useRef, useEffect, useState } from "react";
import { loadPose, drawSkeleton } from "@/lib/pose";
import { extractPoses, type PoseTimeline } from "@/lib/videoPoseExtractor";
import { comparePoses, normalizePose, findClosestFrame } from "@/lib/poseComparison";

export function PoseComparisonDemo() {
  const [refTimeline, setRefTimeline] = useState<PoseTimeline | null>(null);
  const [matchScore, setMatchScore] = useState(0);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);

  // Extract reference poses from video
  useEffect(() => {
    extractPoses("/api/video/abc123", (progress) => {
      console.log(`Extracting: ${progress}%`);
    }).then(setRefTimeline);
  }, []);

  // Set up live pose detection
  useEffect(() => {
    if (!refTimeline) return;

    let pose: any;
    let animationId: number;

    async function init() {
      pose = await loadPose();

      // Set up webcam
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      const video = document.createElement("video");
      video.srcObject = stream;
      await video.play();

      pose.onResults((results: any) => {
        if (!results.poseLandmarks) return;

        // Get current reference frame
        const videoTime = videoRef.current?.currentTime || 0;
        const refFrame = findClosestFrame(refTimeline, videoTime);
        if (!refFrame) return;

        // Normalize and compare
        const refNorm = normalizePose(refFrame.landmarks, 16/9);
        const liveNorm = normalizePose(results.poseLandmarks, 4/3);
        const { overallScore, connectionColors } = comparePoses(refNorm, liveNorm);

        setMatchScore(overallScore);

        // Draw with color-coded feedback
        const ctx = canvasRef.current!.getContext("2d")!;
        drawSkeleton(
          ctx,
          results.poseLandmarks,
          canvasRef.current!.width,
          canvasRef.current!.height,
          { connectionColors }
        );
      });

      function processFrame() {
        pose.send({ image: video });
        animationId = requestAnimationFrame(processFrame);
      }
      processFrame();
    }

    init();

    return () => {
      cancelAnimationFrame(animationId);
      pose?.close();
    };
  }, [refTimeline]);

  return (
    <div>
      <h2>Match Score: {matchScore}%</h2>
      <canvas ref={canvasRef} width={640} height={480} />
      <video ref={videoRef} src="/api/video/abc123" controls />
    </div>
  );
}
```

## Aspect Ratio Guide

**Common aspect ratios:**

```typescript
const ASPECT_RATIOS = {
  "16:9": 16/9,      // 1.778 (YouTube, modern video)
  "4:3": 4/3,        // 1.333 (older webcams)
  "1:1": 1,          // 1.000 (square, Instagram)
  "9:16": 9/16,      // 0.562 (vertical video, TikTok)
};
```

**Why aspect ratio matters:**

MediaPipe returns x and y in [0, 1] relative to image dimensions. A 16:9 video has wider horizontal range than a 4:3 video:

```
16:9 video: x spans 1920px, y spans 1080px
4:3 video:  x spans 640px,  y spans 480px

Without correction:
  16:9 x=0.5 → 960px (center)
  4:3 x=0.5 → 320px (center)
  
  But physically, an arm at x=0.7 in 16:9 is further right
  than an arm at x=0.7 in 4:3!

With correction:
  correctedX = x * aspectRatio
  16:9 x=0.7 → 0.7 * 1.778 = 1.245
  4:3 x=0.7 → 0.7 * 1.333 = 0.933
  
  Now comparable in normalized space!
```

## Performance Notes

- **normalizePose**: ~0.1ms per frame (negligible)
- **comparePoses**: ~0.5ms per frame (safe to run at 60 FPS)
- **comparePosesDetailed**: ~0.6ms per frame (includes classification)
- **findClosestFrame**: O(n) linear search, ~0.01ms for 100-frame timeline

## Best Practices

1. **Always normalize before comparison**: Use `normalizePose()` with correct aspect ratios
2. **Handle null frames**: `findClosestFrame()` returns null if no valid poses
3. **Visibility filtering**: Both functions skip landmarks with visibility < 0.3
4. **Reference-centric logic**: Missing reference landmarks are ignored; missing user landmarks are penalized
5. **Color-coded feedback**: Pass `connectionColors` to `drawSkeleton()` for visual feedback

## Debugging Tips

**Visualize normalized poses:**
```typescript
const refNorm = normalizePose(refLandmarks, 16/9);
const liveNorm = normalizePose(liveLandmarks, 4/3);

// Draw both on the same canvas
drawSkeleton(ctx, refNorm, width, height, { strokeColor: "blue", clear: true });
drawSkeleton(ctx, liveNorm, width, height, { strokeColor: "red", clear: false });
// Blue (reference) and red (live) should overlap if poses match
```

**Log per-limb scores:**
```typescript
const detailed = comparePosesDetailed(refLandmarks, liveLandmarks, 16/9, 4/3);
if (detailed) {
  console.table(detailed.limbScores);
  // Shows which limbs need work
}
```